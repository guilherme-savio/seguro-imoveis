{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introdu\u00e7\u00e3o","text":"<p>Esse projeto foi desenvolvido para a disciplina de Engenharia de Dados, que consta no desenvolvimento completo de uma pipeline de dados para um sistema de seguro de im\u00f3veis, partindo da cria\u00e7\u00e3o de ambientes em cloud utilizando IaC (Terraform), seguindo para os processos de ingest\u00e3o, transforma\u00e7\u00e3o e carregamento dos dados utilizando Azure Databricks e Azure Datalake Storage Gen2. Ao final da pipeline, os dados manipulados s\u00e3o exibidos em um dashboard feito com Power BI.  </p>"},{"location":"#comecando","title":"Come\u00e7ando","text":"<p>Essas instru\u00e7\u00f5es permitir\u00e3o que voc\u00ea obtenha uma c\u00f3pia do projeto em opera\u00e7\u00e3o na sua m\u00e1quina local para fins de desenvolvimento e teste.</p>"},{"location":"#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<ul> <li>Terraform</li> <li>Conta Microsoft/Azure com assinatura paga</li> <li>Python</li> </ul> <p>Conta Microsoft/Azure</p> <p>Existe a possibilidade de adquirir 14 dias gratuitos dos servi\u00e7os premium ofertados pela Microsoft/Azure Verifique a disponibilidade no site</p>"},{"location":"#instalacao","title":"Instala\u00e7\u00e3o","text":"<ol> <li> <p>Clone o reposit\u00f3rio</p> <pre><code>git clone https://github.com/guilherme-savio/seguro-imoveis.git\n</code></pre> </li> <li> <p>Com sua conta Microsoft/Azure criada e apta para uso dos recursos pagos, no Portal Azure crie um workspace Azure Databricks seguindo a documenta\u00e7\u00e3o fornecida pela Microsoft. Durante a execu\u00e7\u00e3o deste processo, voc\u00ea ir\u00e1 criar um <code>resource group</code>. Salve o nome informado no <code>resource group</code> pois ele ser\u00e1 utilizado logo em seguida.</p> </li> <li> <p>Com o Terraform instalado e o resource group em m\u00e3os, no arquivo <code>/iac/variables.tf</code> troque a v\u00e1riavel <code>\"resource_group_name\"</code> para resource group que voc\u00ea criou previamente.</p> </li> <li> <p>Nesta etapa, iremos iniciar o deploy do nosso ambiente cloud. Ap\u00f3s alterar a vari\u00e1vel no \u00faltimo passo, acesse a pasta <code>/iac</code> e execute os seguintes comandos:     <pre><code>terraform init\n</code></pre> <pre><code>terraform apply\n</code></pre></p> </li> <li> <p>Com a execu\u00e7\u00e3o dos comandos finalizada, verifique no Portal Azure o <code>MS SQL Server</code>, <code>MS SQL Database</code> e o <code>ADLS Gen2</code> contendo os containers <code>landing-zone</code>, <code>bronze</code>, <code>silver</code> e <code>gold</code> que foram criados no passo anterior. </p> </li> <li> <p>No Portal Azure, gere um <code>SAS TOKEN</code> para o cont\u00eainer <code>landing-zone</code> seguindo esta documenta\u00e7\u00e3o. Guarde este token em um local seguro pois ele ser\u00e1 utilizado no pr\u00f3ximo passo. </p> </li> <li> <p>Na pasta <code>/data</code>, crie um arquivo chamado <code>.env</code> com o mesmo conte\u00fado disponibilizado no arquivo de exemplo <code>.env.example</code> e preencha as informa\u00e7\u00f5es necess\u00e1rias.</p> </li> <li> <p>No mesmo diret\u00f3rio, vamos iniciar o processo de popula\u00e7\u00e3o do nosso banco de dados. Verifique corretamente o preenchimento das v\u00e1riaveis no arquivo <code>.env</code> e prossiga com os seguintes comandos:</p> <ul> <li>Criar <code>venv</code> (ambiente virtual) do Python:         <pre><code>python3 -m venv env\n</code></pre></li> <li>Ativar a <code>venv</code> criada:<ul> <li>Linux/MacOS:     <pre><code>source env/bin/activate\n</code></pre></li> <li>Windows:     <pre><code>.env\\Scripts\\activate\n</code></pre></li> </ul> </li> <li>Instalar os pacotes necess\u00e1rios:     <pre><code>pip install -r requirements.txt\n</code></pre></li> <li>Executar o script de popula\u00e7\u00e3o:     <pre><code>python -B main.py\n</code></pre></li> </ul> </li> <li>Acesse o <code>Portal Azure</code> e acesse o seu workspace Azure Databricks. Realize o upload dos notebooks encontrados em <code>/etl</code> para o workspace.</li> <li>Por fim, voc\u00ea pode execut\u00e1-los separadamente ou elaborar um Job para orquestrar \u00e0s execu\u00e7\u00f5es.</li> </ol>"},{"location":"#ferramentas-utilizadas","title":"Ferramentas utilizadas","text":"<ul> <li>Terraform - Automa\u00e7\u00e3o de infraestrutura para provisionar e gerenciar recursos em qualquer nuvem ou data center.</li> <li>Azure Databricks - An\u00e1lise e processamento de Big Data</li> <li>Azure SQL Server - Sistema de gerenciamento de banco de dados relacional</li> <li>Azure Datalake Storage Gen2 - Plataforma para armazenar, gerenciar e analisar dados na nuvem</li> </ul>"},{"location":"arquitetura/","title":"Arquitetura de Solu\u00e7\u00e3o","text":"<p>A pipeline \u00e9 constru\u00edda em cima do core de ferramentas de an\u00e1lise de dados e engenharia de dados da Azure. Por estar centralizada em apenas um ambiente cloud, todo o processo de deploy, armazenamento, orquestramento do fluxo de processamento dos dados e custos se torna simples de ser feito, sendo esse um ponto chave para a utiliza\u00e7\u00e3o.</p> <p></p> <p>O processo conta com os seguintes componentes:</p> <ul> <li><code>Origem</code>: Representa os dados do ambiente relacional que est\u00e3o armazenados em um banco de dados <code>Azure SQL Server</code>;</li> <li><code>Ingest\u00e3o</code>: Etapa em que os dados do ambiente relacional s\u00e3o salvos no formato .parquet no container landing-zone por meio do <code>Azure Databricks</code> e <code>Apache Spark</code>;</li> <li><code>Processamento e Orquestra\u00e7\u00e3o</code>: Exp\u00f5e as ferramentas (anteriormente citadas) respons\u00e1veis por essas tarefas;</li> <li><code>Armazenamento</code>: Representa a estrutura de armazenamento do <code>Azure Datalake Storage Gen2</code> da pipeline. Indica o fluxo de processamento, os containeres existentes na nuvem e as tecnologias usadas;</li> <li><code>An\u00e1lise</code>: Simboliza o ambiente final onde ocorre o consumo e an\u00e1lise dos dados tratados em um dashboard criado com <code>Power BI</code>;</li> </ul>"},{"location":"arquitetura/#origem","title":"Origem","text":"<p>O ambiente relacional indicado, vem de um banco de dados Azure SQL Server criado previamente com Terraform.</p> <pre><code>Exemplo do SQL Server e SQL Database que s\u00e3o criados:\n\nresource \"azurerm_mssql_server\" \"sql\" {\n  name                         = \"satcseguroimoveissqlserver\"\n  resource_group_name          = var.resource_group_name\n  location                     = var.resource_group_location\n  version                      = \"12.0\"\n  administrator_login          = var.mssql_admin\n  administrator_login_password = var.password\n}\n\nresource \"azurerm_mssql_database\" \"sql\" {\n  name                        = \"satcseguroimoveisdatabase\"\n  server_id                   = azurerm_mssql_server.sql.id\n  collation                   = \"SQL_Latin1_General_CP1_CI_AS\"\n  auto_pause_delay_in_minutes = -1\n  max_size_gb                 = 2\n  read_replica_count          = 0\n  read_scale                  = false\n  sku_name                    = \"Basic\"\n  zone_redundant              = false\n  geo_backup_enabled          = false\n}\n</code></pre> <p>Para mais informa\u00e7\u00f5es a respeito do formato de tabelas ou da origem dos dados analisados, voc\u00ea pode conferir na documenta\u00e7\u00e3o aqui!</p>"},{"location":"arquitetura/#ingestao","title":"Ingest\u00e3o","text":"<p>O processo de ingest\u00e3o ocorre internamente no Azure Databricks, por meio de um Jupyter Notebook que transforma os dados da origem para o formato .parquet, armazenando no container landing-zone. Para mais informa\u00e7\u00f5es a respeito do script de ingest\u00e3o voc\u00ea pode conferir aqui!</p>"},{"location":"arquitetura/#processamento-e-orquestracao","title":"Processamento e Orquestra\u00e7\u00e3o","text":"<p>Tamb\u00e9m gerenciado pelo Azure Databricks, as tarefas de processamento s\u00e3o orquestradas para acontecerem em cadeia, dependendo do sucesso da execu\u00e7\u00e3o da tarefa anterior. Partindo do container landing-zone, os restantes utilizam o formato de armazenamento de tabelas <code>Delta</code>, caracterizando o ADLS como um <code>Deltalake</code>, garantindo a atomicidade, consist\u00eancia, isolamento e durabilidade das opera\u00e7\u00f5es.</p>"},{"location":"arquitetura/#armazenamento","title":"Armazenamento","text":"<p>O armazenamento de dados ocorre num blob storage centralizado para todos os dados que foram coletados, que est\u00e3o em processamento e que j\u00e1 est\u00e3o dispon\u00edveis para an\u00e1lise. Orientado \u00e0 arquitetura medalh\u00e3o, \u00e9 dividido em 4 containers, sendo eles:</p> <ul> <li><code>landing-zone</code>: Camada respons\u00e1vel pelo armazenamento dos dados brutos oriundos do ambiente relacional no formato parquet;</li> <li><code>bronze</code>: Os dados da camada landing-zone no formato de Delta-table com hist\u00f3rico de extra\u00e7\u00e3o e registro do arquivo de origem.</li> <li><code>silver</code>: Os dados da camada bronze no formato de Delta-table com desnormaliza\u00e7\u00f5es, hist\u00f3rico de extra\u00e7\u00e3o e registro do tabela delta de origem;</li> <li><code>gold</code>: Os dados da camada silver no formato de Delta-table completamete desnormalizados e organizados numa OBT (One Big Table) baseado nas m\u00e9tricas e KPI's requisitados para an\u00e1lise;</li> </ul>"},{"location":"arquitetura/#analise","title":"An\u00e1lise","text":"<p>O componente de an\u00e1lise \u00e9 respons\u00e1vel por exibir num dashboard os dados tratados que est\u00e3o na camada gold. Pelo PowerBI, os consumidores visualizam as m\u00e9tricas e KPI's necessitados.</p>"},{"location":"bronze/","title":"Documenta\u00e7\u00e3o da Camada Bronze","text":""},{"location":"bronze/#visao-geral","title":"Vis\u00e3o Geral","text":"<p>A camada Bronze serve como o armazenamento de dados brutos em nosso pipeline de dados. Esta camada \u00e9 projetada para capturar dados em sua forma bruta, enriquecida com metadados para fins de rastreamento e auditoria. Os dados da zona de aterrissagem (landing-zone) s\u00e3o ingeridos na camada Bronze no formato Delta.</p>"},{"location":"bronze/#conta-de-armazenamento","title":"Conta de Armazenamento","text":"<ul> <li>Nome da Conta de Armazenamento: <code>satcseguroimoveis</code></li> </ul>"},{"location":"bronze/#tabelas-ingeridas","title":"Tabelas Ingeridas","text":"<p>As seguintes tabelas s\u00e3o ingeridas da landing-zone e armazenadas na camada Bronze: - <code>apolice_cobertura</code> - <code>cobertura</code> - <code>sinistro</code> - <code>pagamento</code> - <code>apolice</code> - <code>avaliacao</code> - <code>imovel</code> - <code>cliente</code></p>"},{"location":"bronze/#processo-de-ingestao-de-dados","title":"Processo de Ingest\u00e3o de Dados","text":"<p>O processo de ingest\u00e3o de dados envolve os seguintes passos:</p> <ol> <li>Leitura dos Dados: Os dados s\u00e3o lidos da landing-zone no formato Parquet.</li> <li>Adi\u00e7\u00e3o de Metadados:</li> <li><code>dt_insert_bronze</code>: Timestamp de quando os dados foram inseridos na camada Bronze.</li> <li><code>filename</code>: Nome da tabela de origem.</li> <li>Grava\u00e7\u00e3o dos Dados: Os dados s\u00e3o gravados na camada Bronze no formato Delta.</li> </ol>"},{"location":"bronze/#codigo-de-exemplo","title":"C\u00f3digo de Exemplo","text":"<pre><code>Abaixo est\u00e1 o c\u00f3digo PySpark utilizado para ler os dados da landing-zone, adicionar metadados e grav\u00e1-los na camada Bronze.\n\n```python\nfrom pyspark.sql.functions import current_timestamp, lit\n\nstorageAccountName = \"satcseguroimoveis\"\ntables = [\"apolice_cobertura\", \"cobertura\", \"sinistro\", \"pagamento\", \"apolice\", \"avaliacao\", \"imovel\", \"cliente\"]\n\nfor table in tables:\n    remote_table = spark.read.option(\"inferschema\", \"true\").option(\"header\", \"true\").parquet(f\"/mnt/{storageAccountName}/landing-zone/{table}\")\n    remote_table = remote_table.withColumn(\"dt_insert_bronze\", current_timestamp()).withColumn(\"filename\", lit(table))\n    remote_table.write.format('delta').save(f\"/mnt/{storageAccountName}/bronze/{table}\")\n</code></pre>"},{"location":"contributors/","title":"Autores","text":"<ul> <li>Bruno Venturini - Ingest\u00e3o, ETL, DDL</li> <li>Eduardo Freitas - Infraestrutura, ETL</li> <li>Gabriel Della - DDL, Diagramas</li> <li>Gabriel Ferreira - Dashboard, ETL</li> <li>Guilherme Savio - Arquitetura, ETL, Infraestrutura</li> <li>Higor Goulart - Popula\u00e7\u00e3o, ETL</li> <li>Sofia Martins- Documenta\u00e7\u00e3o, ETL, Dashboard</li> </ul>"},{"location":"etl/","title":"ETL","text":""},{"location":"relational/","title":"Modelo Relacional e Inser\u00e7\u00e3o de Dados","text":"<p>A estrutura incial do banco de dados do projeto cont\u00e9m 8 tabelas, sendo elas as seguintes:</p> <ul> <li><code>cliente</code>: armazena informa\u00e7\u00f5es dos clientes;</li> <li><code>imovel</code>: armazena informa\u00e7\u00f5es dos im\u00f3veis, relacionando-os com propriet\u00e1rios e inquilinos que s\u00e3o clientes;</li> <li><code>apolice</code>: armazena informa\u00e7\u00f5es das ap\u00f3lices de seguros associadas a im\u00f3veis;</li> <li><code>cobertura</code>: armazena informa\u00e7\u00f5es das coberturas de seguro;</li> <li><code>apolice_cobertura</code>: associa\u00e7\u00e3o para o relacionamento muitos-para-muitos entre ap\u00f3lices e coberturas;</li> <li><code>sinistro</code>: armazena informa\u00e7\u00f5es dos sinistros associados a ap\u00f3lices;</li> <li><code>pagamento</code>: armazena informa\u00e7\u00f5es dos pagamentos associados a ap\u00f3lices;</li> <li><code>avaliacao</code>: armazena informa\u00e7\u00f5es das avalia\u00e7\u00f5es dos im\u00f3veis;</li> </ul> <p></p>"},{"location":"relational/#insercao-de-dados","title":"Inser\u00e7\u00e3o de Dados","text":"<p>A partir de um script em Python, s\u00e3o criadas as tabelas e populada toda a cadeia de dados para N clientes.</p> <pre><code>total_clients = 100\n\nclientes = insert_clientes(session, total_clients)\nlog(f\"{len(clientes)} clientes inseridos.\")\n\nimoveis = insert_imoveis(session, clientes, tipos_imovel)\nlog(f\"{len(imoveis)} im\u00f3veis inseridos.\")\n\napolices = insert_avaliacoes_apolices(session, imoveis)\nlog(f\"{len(apolices)} ap\u00f3lices e avalia\u00e7\u00f5es inseridas.\")\n\nsinistros, pagamentos, apolice_coberturas = await insert_sinistros_pagamentos_apolices_coberturas(session, apolices, imoveis, coberturas)\nlog(f\"{len(sinistros)} sinistros, {len(pagamentos)} pagamentos, {len(apolice_coberturas)} apolice_coberturas inseridos.\")\n\nsession.commit()\nlog(\"Todos registros salvos.\")\n</code></pre> <p>Para cada cliente, podem existir de 1 a 4 im\u00f3veis. A partir desses im\u00f3veis, obt\u00e9m-se a mesma quantidade em ap\u00f3lices e avalia\u00e7\u00f5es. Ap\u00f3s isso, por cada ap\u00f3lice, tem-se a cria\u00e7\u00e3o de 0 a 4 sinistros, 1 a 10 tipos de coberturas e 1 pagamento por m\u00eas desde a data de in\u00edcio da ap\u00f3lice at\u00e9 o dia atual.</p> <p>Ao final do processo, a base de dados obteve um total de 337.010 linhas a partir da soma de todas as tabelas.</p> linhas tabela 58332 apolice_cobertura 10 cobertura 18289 sinistro 207253 pagamento 15042 apolice 15042 avaliacao 15042 imovel 8000 cliente"},{"location":"silver/","title":"Documenta\u00e7\u00e3o da Camada Silver","text":"<p>Bem-vindo \u00e0 documenta\u00e7\u00e3o da camada Silver. Aqui voc\u00ea encontrar\u00e1 informa\u00e7\u00f5es sobre as transforma\u00e7\u00f5es realizadas e a refer\u00eancia da API para as fun\u00e7\u00f5es de transforma\u00e7\u00e3o.</p>"},{"location":"silver/#transformacoes-da-camada-silver","title":"Transforma\u00e7\u00f5es da Camada Silver","text":"<p>A camada Silver realiza transforma\u00e7\u00f5es nos dados brutos da camada Bronze para prepar\u00e1-los para a camada Gold. As transforma\u00e7\u00f5es espec\u00edficas realizadas s\u00e3o detalhadas abaixo.</p>"},{"location":"silver/#transformacao-de-cliente","title":"Transforma\u00e7\u00e3o de Cliente","text":"<p>A transforma\u00e7\u00e3o de cliente inclui: - Manter apenas os 10 \u00faltimos n\u00fameros da coluna <code>telefone</code>. - Transformar <code>dt_nasc</code> em <code>data_nascimento</code>. - Renomear colunas para letras mai\u00fasculas e padronizar prefixos <code>ID_</code> para <code>CODIGO_</code> e <code>DT_</code> para <code>DATA_</code>. - Adicionar colunas <code>FILENAME_BRONZE</code> e <code>DATA_INSERT_SILVER</code>.</p>"},{"location":"silver/#codigo","title":"C\u00f3digo","text":"<pre><code>```python\ndef transformar_cliente():\ndf_cliente = spark.read.format(\"delta\").load(f\"/mnt/{storageAccountName}/bronze/cliente\")\n\ndf_cliente = df_cliente.withColumn(\"telefone\", regexp_replace(\"telefone\", \"[^0-9]\", \"\"))\ndf_cliente = df_cliente.withColumn(\"telefone\", col(\"telefone\").substr(-10, 10))\ndf_cliente = df_cliente.withColumnRenamed(\"dt_nasc\", \"data_nascimento\")\n\ndf = renomear_colunas(df_cliente, \"cliente\")\n\nsalvar_silver(df, \"cliente\")\n</code></pre>"},{"location":"silver/#tabela-de-conversao-silver","title":"Tabela de convers\u00e3o silver","text":"CODIGO_APOLICE CODIGO_COBERTURA FILENAME_BRONZE DATA_INSERT_SILVER 1 5 apolice_cobertura 2024-06-27 03:11:... 1 10 apolice_cobertura 2024-06-27 03:11:... 1 6 apolice_cobertura 2024-06-27 03:11:... 2 1 apolice_cobertura 2024-06-27 03:11:... 2 9 apolice_cobertura 2024-06-27 03:11:... 2 3 apolice_cobertura 2024-06-27 03:11:... 2 8 apolice_cobertura 2024-06-27 03:11:... 3 5 apolice_cobertura 2024-06-27 03:11:... 3 10 apolice_cobertura 2024-06-27 03:11:... 3 1 apolice_cobertura 2024-06-27 03:11:... 3 7 apolice_cobertura 2024-06-27 03:11:... 4 6 apolice_cobertura 2024-06-27 03:11:... 5 2 apolice_cobertura 2024-06-27 03:11:... 5 9 apolice_cobertura 2024-06-27 03:11:... 5 1 apolice_cobertura 2024-06-27 03:11:..."}]}