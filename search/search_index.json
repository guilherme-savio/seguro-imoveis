{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introdu\u00e7\u00e3o","text":"<p>Esse projeto foi desenvolvido para a disciplina de Engenharia de Dados, que consta no desenvolvimento completo de uma pipeline de dados para um sistema de seguro de im\u00f3veis, partindo da cria\u00e7\u00e3o de ambientes em cloud utilizando IaC (Terraform), seguindo para os processos de ingest\u00e3o, transforma\u00e7\u00e3o e carregamento dos dados utilizando Azure Databricks e Azure Datalake Storage Gen2. Ao final da pipeline, os dados manipulados s\u00e3o exibidos em um dashboard feito com Power BI.  </p>"},{"location":"#comecando","title":"Come\u00e7ando","text":"<p>Essas instru\u00e7\u00f5es permitir\u00e3o que voc\u00ea obtenha uma c\u00f3pia do projeto em opera\u00e7\u00e3o na sua m\u00e1quina local para fins de desenvolvimento e teste.</p>"},{"location":"#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<ul> <li>Terraform</li> <li>Conta Microsoft/Azure com assinatura paga</li> <li>Python</li> </ul> <p>Conta Microsoft/Azure</p> <p>Existe a possibilidade de adquirir 14 dias gratuitos dos servi\u00e7os premium ofertados pela Microsoft/Azure Verifique a disponibilidade no site</p>"},{"location":"#instalacao","title":"Instala\u00e7\u00e3o","text":"<ol> <li> <p>Clone o reposit\u00f3rio</p> <pre><code>git clone https://github.com/guilherme-savio/seguro-imoveis.git\n</code></pre> </li> <li> <p>Com sua conta Microsoft/Azure criada e apta para uso dos recursos pagos, no Portal Azure crie um workspace Azure Databricks seguindo a documenta\u00e7\u00e3o fornecida pela Microsoft. Durante a execu\u00e7\u00e3o deste processo, voc\u00ea ir\u00e1 criar um <code>resource group</code>. Salve o nome informado no <code>resource group</code> pois ele ser\u00e1 utilizado logo em seguida.</p> </li> <li> <p>Com o Terraform instalado e o resource group em m\u00e3os, no arquivo <code>/iac/variables.tf</code> troque a v\u00e1riavel <code>\"resource_group_name\"</code> para resource group que voc\u00ea criou previamente.</p> </li> <li> <p>Nesta etapa, iremos iniciar o deploy do nosso ambiente cloud. Ap\u00f3s alterar a vari\u00e1vel no \u00faltimo passo, acesse a pasta <code>/iac</code> e execute os seguintes comandos:     <pre><code>terraform init\n</code></pre> <pre><code>terraform apply\n</code></pre></p> </li> <li> <p>Com a execu\u00e7\u00e3o dos comandos finalizada, verifique no Portal Azure o <code>MS SQL Server</code>, <code>MS SQL Database</code> e o <code>ADLS Gen2</code> contendo os containers <code>landing-zone</code>, <code>bronze</code>, <code>silver</code> e <code>gold</code> que foram criados no passo anterior. </p> </li> <li> <p>No Portal Azure, gere um <code>SAS TOKEN</code> para o cont\u00eainer <code>landing-zone</code> seguindo esta documenta\u00e7\u00e3o. Guarde este token em um local seguro pois ele ser\u00e1 utilizado no pr\u00f3ximo passo. </p> </li> <li> <p>Na pasta <code>/data</code>, crie um arquivo chamado <code>.env</code> com o mesmo conte\u00fado disponibilizado no arquivo de exemplo <code>.env.example</code> e preencha as informa\u00e7\u00f5es necess\u00e1rias.</p> </li> <li> <p>No mesmo diret\u00f3rio, vamos iniciar o processo de popula\u00e7\u00e3o do nosso banco de dados. Verifique corretamente o preenchimento das v\u00e1riaveis no arquivo <code>.env</code> e prossiga com os seguintes comandos:</p> <ul> <li>Criar <code>venv</code> (ambiente virtual) do Python:         <pre><code>python3 -m venv env\n</code></pre></li> <li>Ativar a <code>venv</code> criada:<ul> <li>Linux/MacOS:     <pre><code>source env/bin/activate\n</code></pre></li> <li>Windows:     <pre><code>.env\\Scripts\\activate\n</code></pre></li> </ul> </li> <li>Instalar os pacotes necess\u00e1rios:     <pre><code>pip install -r requirements.txt\n</code></pre></li> <li>Executar o script de popula\u00e7\u00e3o:     <pre><code>python -B main.py\n</code></pre></li> </ul> </li> <li>Acesse o <code>Portal Azure</code> e acesse o seu workspace Azure Databricks. Realize o upload dos notebooks encontrados em <code>/etl</code> para o workspace.</li> <li>Por fim, voc\u00ea pode execut\u00e1-los separadamente ou elaborar um Job para orquestrar \u00e0s execu\u00e7\u00f5es.</li> </ol>"},{"location":"#ferramentas-utilizadas","title":"Ferramentas utilizadas","text":"<ul> <li>Terraform - Automa\u00e7\u00e3o de infraestrutura para provisionar e gerenciar recursos em qualquer nuvem ou data center.</li> <li>Azure Databricks - An\u00e1lise e processamento de Big Data</li> <li>Azure SQL Server - Sistema de gerenciamento de banco de dados relacional</li> <li>Azure Datalake Storage Gen2 - Plataforma para armazenar, gerenciar e analisar dados na nuvem</li> </ul>"},{"location":"arquitetura/","title":"Arquitetura de Solu\u00e7\u00e3o","text":"<p>A pipeline \u00e9 constru\u00edda em cima do core de ferramentas de an\u00e1lise de dados e engenharia de dados da Azure. Por estar centralizada em apenas um ambiente cloud, todo o processo de deploy, armazenamento, orquestramento do fluxo de processamento dos dados e custos se torna simples de ser feito, sendo esse um ponto chave para a utiliza\u00e7\u00e3o.</p> <p></p> <p>O processo conta com os seguintes componentes:</p> <ul> <li><code>Origem</code>: Representa os dados do ambiente relacional que est\u00e3o armazenados em um banco de dados <code>Azure SQL Server</code>;</li> <li><code>Ingest\u00e3o</code>: Etapa em que os dados do ambiente relacional s\u00e3o salvos no formato .parquet no container landing-zone por meio do <code>Azure Databricks</code> e <code>Apache Spark</code>;</li> <li><code>Processamento e Orquestra\u00e7\u00e3o</code>: Exp\u00f5e as ferramentas (anteriormente citadas) respons\u00e1veis por essas tarefas;</li> <li><code>Armazenamento</code>: Representa a estrutura de armazenamento do <code>Azure Datalake Storage Gen2</code> da pipeline. Indica o fluxo de processamento, os containeres existentes na nuvem e as tecnologias usadas;</li> <li><code>An\u00e1lise</code>: Simboliza o ambiente final onde ocorre o consumo e an\u00e1lise dos dados tratados em um dashboard criado com <code>Power BI</code>;</li> </ul>"},{"location":"arquitetura/#origem","title":"Origem","text":"<p>O ambiente relacional indicado, vem de um banco de dados Azure SQL Server criado previamente com Terraform.</p> <pre><code>Exemplo do SQL Server e SQL Database que s\u00e3o criados:\n\nresource \"azurerm_mssql_server\" \"sql\" {\n  name                         = \"satcseguroimoveissqlserver\"\n  resource_group_name          = var.resource_group_name\n  location                     = var.resource_group_location\n  version                      = \"12.0\"\n  administrator_login          = var.mssql_admin\n  administrator_login_password = var.password\n}\n\nresource \"azurerm_mssql_database\" \"sql\" {\n  name                        = \"satcseguroimoveisdatabase\"\n  server_id                   = azurerm_mssql_server.sql.id\n  collation                   = \"SQL_Latin1_General_CP1_CI_AS\"\n  auto_pause_delay_in_minutes = -1\n  max_size_gb                 = 2\n  read_replica_count          = 0\n  read_scale                  = false\n  sku_name                    = \"Basic\"\n  zone_redundant              = false\n  geo_backup_enabled          = false\n}\n</code></pre> <p>Para mais informa\u00e7\u00f5es a respeito do formato de tabelas ou da origem dos dados analisados, voc\u00ea pode conferir na documenta\u00e7\u00e3o aqui!</p>"},{"location":"arquitetura/#ingestao","title":"Ingest\u00e3o","text":"<p>O processo de ingest\u00e3o ocorre internamente no Azure Databricks, por meio de um Jupyter Notebook que transforma os dados da origem para o formato .parquet, armazenando no container landing-zone. Para mais informa\u00e7\u00f5es a respeito do script de ingest\u00e3o voc\u00ea pode conferir aqui!</p>"},{"location":"arquitetura/#processamento-e-orquestracao","title":"Processamento e Orquestra\u00e7\u00e3o","text":"<p>Tamb\u00e9m gerenciado pelo Azure Databricks, as tarefas de processamento s\u00e3o orquestradas para acontecerem em cadeia, dependendo do sucesso da execu\u00e7\u00e3o da tarefa anterior. Partindo do container landing-zone, os restantes utilizam o formato de armazenamento de tabelas <code>Delta</code>, caracterizando o ADLS como um <code>Deltalake</code>, garantindo a atomicidade, consist\u00eancia, isolamento e durabilidade das opera\u00e7\u00f5es.</p>"},{"location":"arquitetura/#armazenamento","title":"Armazenamento","text":"<p>O armazenamento de dados ocorre num blob storage centralizado para todos os dados que foram coletados, que est\u00e3o em processamento e que j\u00e1 est\u00e3o dispon\u00edveis para an\u00e1lise. Orientado \u00e0 arquitetura medalh\u00e3o, \u00e9 dividido em 4 containers, sendo eles:</p> <ul> <li><code>landing-zone</code>: Camada respons\u00e1vel pelo armazenamento dos dados brutos oriundos do ambiente relacional no formato parquet;</li> <li><code>bronze</code>: Os dados da camada landing-zone no formato de Delta-table com hist\u00f3rico de extra\u00e7\u00e3o e registro do arquivo de origem.</li> <li><code>silver</code>: Os dados da camada bronze no formato de Delta-table com desnormaliza\u00e7\u00f5es, hist\u00f3rico de extra\u00e7\u00e3o e registro do tabela delta de origem;</li> <li><code>gold</code>: Os dados da camada silver no formato de Delta-table completamete desnormalizados e organizados numa OBT (One Big Table) baseado nas m\u00e9tricas e KPI's requisitados para an\u00e1lise;</li> </ul>"},{"location":"arquitetura/#analise","title":"An\u00e1lise","text":"<p>O componente de an\u00e1lise \u00e9 respons\u00e1vel por exibir num dashboard os dados tratados que est\u00e3o na camada gold. Pelo PowerBI, os consumidores visualizam as m\u00e9tricas e KPI's necessitados.</p>"},{"location":"contributors/","title":"Autores","text":"<ul> <li>Bruno Venturini - Ingest\u00e3o, ETL, DDL</li> <li>Eduardo Freitas - Infraestrutura, ETL</li> <li>Gabriel Della - DDL, Diagramas</li> <li>Gabriel Ferreira - Dashboard, ETL</li> <li>Guilherme Savio - Arquitetura, ETL, Infraestrutura</li> <li>Higor Goulart - Popula\u00e7\u00e3o, ETL</li> <li>Sofia Martins- Documenta\u00e7\u00e3o, ETL, Dashboard</li> </ul>"},{"location":"etl/","title":"ETL","text":""},{"location":"obt/","title":"Camada Gold - One Big table","text":""},{"location":"obt/#estrutura-das-tabelas","title":"Estrutura das Tabelas","text":"<p>O ambiente consiste em 6 tabelas principais que foram carregadas do armazenamento Delta na camada silver:</p> <ul> <li><code>apolice</code>: Cont\u00e9m informa\u00e7\u00f5es sobre as ap\u00f3lices de seguros.</li> <li><code>sinistro</code>: Cont\u00e9m informa\u00e7\u00f5es sobre os sinistros associados \u00e0s ap\u00f3lices.</li> <li><code>imovel</code>: Cont\u00e9m informa\u00e7\u00f5es sobre os im\u00f3veis assegurados.</li> <li><code>apolice_cobertura</code>: Tabela de associa\u00e7\u00e3o que gerencia o relacionamento muitos-para-muitos entre ap\u00f3lices e coberturas.</li> <li><code>cobertura</code>: Cont\u00e9m informa\u00e7\u00f5es sobre os diferentes tipos de coberturas de seguro.</li> <li><code>avaliacao</code>: Cont\u00e9m avalia\u00e7\u00f5es dos im\u00f3veis.</li> </ul>"},{"location":"obt/#leitura-dos-dados","title":"Leitura dos Dados","text":"<p>Os dados s\u00e3o lidos do armazenamento Delta da camada silver e armazenados em DataFrames do PySpark.</p> <pre><code>from pyspark.sql.functions import year, month, sum, count, avg, format_string, date_format, coalesce, expr, lit\n\nstorageAccountName = \"satcseguroimoveis\"\n\napolice_df = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/apolice\")\nsinistro_df = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/sinistro\")\nimovel_df = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/imovel\")\napolice_cobertura_df = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/apolice_cobertura\")\ncobertura_df = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/cobertura\")\navaliacao_df = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/avaliacao\")\n</code></pre>"},{"location":"obt/#escrita-dos-dados","title":"Escrita dos Dados","text":"<p>Os DataFrames s\u00e3o ent\u00e3o escritos em tabelas Delta para utiliza\u00e7\u00e3o futura.</p> <pre><code>apolice_df.write.format('delta').mode('overwrite').saveAsTable(\"APOLICE\")\nsinistro_df.write.format('delta').mode('overwrite').saveAsTable(\"SINISTRO\")\nimovel_df.write.format('delta').mode('overwrite').saveAsTable(\"IMOVEL\")\napolice_cobertura_df.write.format('delta').mode('overwrite').saveAsTable(\"APOLICE_COBERTURA\")\ncobertura_df.write.format('delta').mode('overwrite').saveAsTable(\"COBERTURA\")\navaliacao_df.write.format('delta').mode('overwrite').saveAsTable(\"AVALIACAO\")\n</code></pre>"},{"location":"obt/#criacao-da-tabela-obt-operational-business-table","title":"Cria\u00e7\u00e3o da Tabela OBT (Operational Business Table)","text":"<p>A tabela OBT \u00e9 criada para consolidar as informa\u00e7\u00f5es agregadas e gerar insights de neg\u00f3cios.</p> <pre><code>CREATE OR REPLACE TABLE satcseguroimoveis_obt (\n    ANO INT,\n    MES STRING,\n    TOTAL_DE_VALOR_DE_SINISTRO DECIMAL(38,2) NOT NULL,\n    VALOR_TOTAL_DAS_APOLICES DECIMAL(38,2) NOT NULL,\n    TOTAL_DE_ATIVOS DECIMAL(38,2) NOT NULL,\n    TOTAL_DE_PASSIVOS DECIMAL(38,2) NOT NULL,\n    NUMERO_TOTAL_DE_IMOVEIS INT NOT NULL,\n    APOLICES_FINALIZADAS INT NOT NULL,\n    NUMERO_DE_SINISTROS INT NOT NULL,\n    NUMERO_DE_APOLICES_VENDIDAS INT NOT NULL,\n    VALOR_MEDIO_DE_PREMIO DECIMAL(38,6) NOT NULL\n)\nUSING DELTA;\n</code></pre>"},{"location":"obt/#insercao-de-dados-na-tabela-obt","title":"Inser\u00e7\u00e3o de Dados na Tabela OBT","text":"<p>Os dados s\u00e3o inseridos na tabela OBT a partir das tabelas silver,  utilizando agrega\u00e7\u00f5es e jun\u00e7\u00f5es para consolidar a informa\u00e7\u00e3o.</p> <pre><code>INSERT INTO satcseguroimoveis_obt\nSELECT \n    YEAR(A.DATA_INICIO) AS ANO,\n    DATE_FORMAT(A.DATA_INICIO, 'MMMM') AS MES,\n    COALESCE(SUM(S.VALOR_SINISTRO), 0) AS TOTAL_DE_VALOR_DE_SINISTRO,\n    COALESCE(SUM(A.VALOR_APOLICE), 0) AS VALOR_TOTAL_DAS_APOLICES,\n    COALESCE(SUM(C.VALOR), 0) AS TOTAL_DE_ATIVOS, \n    COALESCE(SUM(I.VALOR_IMOVEL), 0) AS TOTAL_DE_PASSIVOS, \n    COALESCE(COUNT(AV.CODIGO_IMOVEL), 0) AS NUMERO_TOTAL_DE_IMOVEIS,\n    COALESCE(SUM(CASE WHEN A.DATA_TERMINO &lt;= CURRENT_DATE THEN 1 ELSE 0 END), 0) AS APOLICES_FINALIZADAS, \n    COALESCE(COUNT(DISTINCT S.CODIGO_SINISTRO), 0) AS NUMERO_DE_SINISTROS,\n    COALESCE(COUNT(DISTINCT A.CODIGO_APOLICE), 0) AS NUMERO_DE_APOLICES_VENDIDAS,\n    COALESCE(AVG(A.VALOR_APOLICE), 0) AS VALOR_MEDIO_DE_PREMIO\nFROM \n    APOLICE A\nLEFT JOIN \n    SINISTRO S ON A.CODIGO_APOLICE = S.CODIGO_APOLICE\nLEFT JOIN \n    IMOVEL I ON A.CODIGO_IMOVEL = I.CODIGO_IMOVEL\nLEFT JOIN \n    APOLICE_COBERTURA AC ON A.CODIGO_APOLICE = AC.CODIGO_APOLICE\nLEFT JOIN \n    COBERTURA C ON AC.CODIGO_COBERTURA = C.CODIGO_COBERTURA\nLEFT JOIN \n    AVALIACAO AV ON I.CODIGO_IMOVEL = AV.CODIGO_IMOVEL\nGROUP BY \n    YEAR(A.DATA_INICIO), DATE_FORMAT(A.DATA_INICIO, 'MMMM');\n</code></pre>"},{"location":"obt/#exibicao-dos-dados","title":"Exibi\u00e7\u00e3o dos Dados","text":"<p>Os dados da tabela OBT s\u00e3o exibidos para an\u00e1lise e tomada de decis\u00f5es.</p> ANO MES TOTAL_DE_VALOR_DE_SINISTRO VALOR_TOTAL_DAS_APOLICES TOTAL_DE_ATIVOS TOTAL_DE_PASSIVOS NUMERO_TOTAL_DE_IMOVEIS APOLICES_FINALIZADAS NUMERO_DE_SINISTROS NUMERO_DE_APOLICES_VENDIDAS VALOR_MEDIO_DE_PREMIO 2023 Janeiro 50000.00 300000.00 120000.00 1000000.00 500 50 200 150 2000.000000 2023 Fevereiro 45000.00 280000.00 110000.00 950000.00 480 45 180 140 2000.000000 2023 Mar\u00e7o 55000.00 310000.00 125000.00 1050000.00 510 55 220 160 1937.500000 2023 Abril 47000.00 290000.00 115000.00 970000.00 490 48 190 150 1933.333333 2023 Maio 53000.00 320000.00 130000.00 1080000.00 520 60 210 170 1882.352941 2023 Junho 48000.00 300000.00 120000.00 990000.00 500 50 200 150 2000.000000 2023 Julho 46000.00 275000.00 105000.00 920000.00 470 47 180 140 1964.285714 2023 Agosto 52000.00 310000.00 125000.00 1040000.00 510 55 220 160 1937.500000 2023 Setembro 49000.00 295000.00 115000.00 980000.00 495 49 190 150 1966.666667 2023 Outubro 53000.00 320000.00 130000.00 1070000.00 520 60 210 170 1882.352941"},{"location":"relational/","title":"Modelo Relacional e Inser\u00e7\u00e3o de Dados","text":"<p>A estrutura incial do banco de dados do projeto cont\u00e9m 8 tabelas, sendo elas as seguintes:</p> <ul> <li><code>cliente</code>: armazena informa\u00e7\u00f5es dos clientes;</li> <li><code>imovel</code>: armazena informa\u00e7\u00f5es dos im\u00f3veis, relacionando-os com propriet\u00e1rios e inquilinos que s\u00e3o clientes;</li> <li><code>apolice</code>: armazena informa\u00e7\u00f5es das ap\u00f3lices de seguros associadas a im\u00f3veis;</li> <li><code>cobertura</code>: armazena informa\u00e7\u00f5es das coberturas de seguro;</li> <li><code>apolice_cobertura</code>: associa\u00e7\u00e3o para o relacionamento muitos-para-muitos entre ap\u00f3lices e coberturas;</li> <li><code>sinistro</code>: armazena informa\u00e7\u00f5es dos sinistros associados a ap\u00f3lices;</li> <li><code>pagamento</code>: armazena informa\u00e7\u00f5es dos pagamentos associados a ap\u00f3lices;</li> <li><code>avaliacao</code>: armazena informa\u00e7\u00f5es das avalia\u00e7\u00f5es dos im\u00f3veis;</li> </ul> <p></p>"},{"location":"relational/#insercao-de-dados","title":"Inser\u00e7\u00e3o de Dados","text":"<p>A partir de um script em Python, s\u00e3o criadas as tabelas e populada toda a cadeia de dados para N clientes.</p> <pre><code>total_clients = 100\n\nclientes = insert_clientes(session, total_clients)\nlog(f\"{len(clientes)} clientes inseridos.\")\n\nimoveis = insert_imoveis(session, clientes, tipos_imovel)\nlog(f\"{len(imoveis)} im\u00f3veis inseridos.\")\n\napolices = insert_avaliacoes_apolices(session, imoveis)\nlog(f\"{len(apolices)} ap\u00f3lices e avalia\u00e7\u00f5es inseridas.\")\n\nsinistros, pagamentos, apolice_coberturas = await insert_sinistros_pagamentos_apolices_coberturas(session, apolices, imoveis, coberturas)\nlog(f\"{len(sinistros)} sinistros, {len(pagamentos)} pagamentos, {len(apolice_coberturas)} apolice_coberturas inseridos.\")\n\nsession.commit()\nlog(\"Todos registros salvos.\")\n</code></pre> <p>Para cada cliente, podem existir de 1 a 4 im\u00f3veis. A partir desses im\u00f3veis, obt\u00e9m-se a mesma quantidade em ap\u00f3lices e avalia\u00e7\u00f5es. Ap\u00f3s isso, por cada ap\u00f3lice, tem-se a cria\u00e7\u00e3o de 0 a 4 sinistros, 1 a 10 tipos de coberturas e 1 pagamento por m\u00eas desde a data de in\u00edcio da ap\u00f3lice at\u00e9 o dia atual.</p> <p>Ao final do processo, a base de dados obteve um total de 337.010 linhas a partir da soma de todas as tabelas.</p> linhas tabela 58332 apolice_cobertura 10 cobertura 18289 sinistro 207253 pagamento 15042 apolice 15042 avaliacao 15042 imovel 8000 cliente"}]}